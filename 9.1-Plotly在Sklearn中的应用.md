9.1　Plotly在Sklearn中的应用

本章以讲解案例的方式简单介绍Plotly在Sklearn中的应用，包括机器学习中的三大问题：分类、回归和聚类结果的可视化。每个部分详细介绍一个案例，重点讲解如何制图，而对于机器学习的算法仅在必要时作简单说明。读者在阅读本章时不必纠结算法层面的问题，Sklearn是已经封装好的机器学习包，不需要自己编写程序实现的具体算法。

### 9.1.1　分类问题

本案例使用SVM对经典的IRIS数据集（鸢尾花）进行分类，选择鸢尾花的两个特征作为自变量，其类别作为因变量，分类可视化结果如图9-1所示，代码见文件plotly_SVM.py。

上述可视化结果的实现包含两部分，一个是绘制热力图，即背景图；另一个是绘制样本点。绘制样本点与本书前面绘制散点图相同，需要注意的是将颜色与分类标签挂钩，即color=Y，保证同类样本显示同样的颜色，然后使用colorscale = cmap给不同的标签填上不同的颜色。

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/21473765/1644303594489-3321ca1f-9c44-401c-a0a5-84b865c31613.jpeg)

图9-1　使用线性核函数SVM算法三分类结果图

在绘制热力图时，需要使用plotly.graph_objs中的Heatmap函数，函数中的x、y、z三个参数分别对应图像的X轴坐标、Y轴坐标，以及由X轴线与Y轴线相交所得的每个小方块的颜色。如果传递给x参数的矩阵大小是a，传递给y参数的矩阵大小是b，那么z参数就需要大小为a×b的矩阵，这也是对z参数做reshape操作的原因。案例中传递给heatmap函数中参数x、y的是x_与y_，均为等差数列，起始点是第一个特征的最小值-1（第二个特征的最小值-1），终点是第一个特征的最大值+1（第二个特征的最大值+1），步长为0.02；传递给z参数的则是分类得到的结果矩阵，大小为 220×280。实际上背景图就是由 220×280个已经着色的小矩阵组成的，由于分类的结果不同，因此呈现的颜色也不同，代码如下。

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/21473765/1644303594990-ed831b2b-7b33-42e3-a378-bb726055b478.jpeg)

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/21473765/1644303595481-9b40d6f5-7dab-411d-be6d-4a8ab95c487e.jpeg)

### 9.1.2　回归问题

回归的可视化相比分类要简单很多，本案例使用三种模型（RBF、Linear、Polynomial）进行回归实践，并可视化结果，数据使用NumPy随机生成。上述可视化结果的实现包含两部分，一个是对原始数据点的可视化，另一个是对回归曲线的可视化。在本案例中我们将原始点的x与y值传递给p1，将RBF拟合的结果传递给p2，把线性模型拟合的结果传递给p3，把多项式模型拟合的结果传递给p4，再将这4张图叠加就可以绘制出如图9-2所示的回归结果。本案例见文件plotly_SVR.py，代码如下。

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/21473765/1644303596108-57263193-3b19-4990-b260-4e21e4c1cc48.jpeg)

图9-2　三种算法的回归结果

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/21473765/1644303596520-df735921-798b-4dae-b2d3-f4137cb8b169.jpeg)

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/21473765/1644303597008-5e304a8b-908a-4a24-9d4f-259162de19bd.jpeg)

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/21473765/1644303597518-6222514b-e33b-4cb8-91ba-9580221ccce9.jpeg)

### 9.1.3　聚类问题

本案例介绍聚类问题的可视化，案例中比较了K-means算法和MiniBatchKmeans算法的差异，画图部分与9.1.1节有相同之处。聚类结果可视化一般分为两部分，一个是画出聚类中心点，另一个是将同一个类别的数据绘制为同一种颜色。这里仍然使用Scatter函数完成，详细代码见文件plotly_Kmeans.py。

本案例代码分为三部分，第一部分产生聚类所需的数据；第二部分分别使用K-means算法与MiniBatchKmeans算法训练模型；第三部分绘制三张图片，第一张图片是K-means分类结果，第二张图片是MiniBatchKmeans分类结果，第三张图片突出了两种分类结果的差异。下面详细讲解每个部分。

在第一部分中，需要说明一下scikit中的make_blobs方法，这种方法常常被用来生成聚类算法的测试数据，会根据用户指定的特征数量、中心点数量、范围等生成几类数据，使得数据可用于测试聚类算法的效果。make_blobs函数中包含4个参数，n_samples是待生成的样本的总数，n_features是每个样本的特征数，centers表示类别数，cluster_std表示每个类别数据的方差。

在第二部分中，使用生成好的数据调用K-means和MiniBatchKmeans模型进行聚类，并记录运行时间。

第三部分是绘图部分，首先设定三种类别对应的颜色，然后获取cluster_centers_中的聚类中心点，并使用pairwise_distances_argmin函数获取每个数据距离中心点的最短距离。在获取完所需的可视化样本后，使用make_subplots生成一个一行三列的画布，并指定标题，这样就可以在绘制完每一幅图后把图片更新到对应的画布位置。在代码中，绘制一幅图后的代码“fig['layout']['xaxis1'].update”就是在做把图片更新到画布上这件事情。生成画布后，使用 k循环每次完成一个类别的绘制，kmeans1中绘制所有同一个类别的样本点，而 kmeans2中绘制类别的中心点，最后 fig对应的（1,1）在画布上其实是6张图片的叠加，有三张图片依次绘制了三种不同的类别，另外三张图片依次绘制了各个类别的中心点，MiniBatchKmeans算法结果的可视化与k-means算法结果的可视化相同。最后一张图片是对算法结果的差异展示，首先通过逻辑判断找出两种聚类算法结果不同的样本点并在difference2中绘制，然后使用numpy.logical_not方法选出相同的样本数并在difference1中绘制。最后三幅图的绘制结果如图9-3所示。

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/21473765/1644303598132-496f66cc-3a6d-46a6-a198-4d02f6ced9d9.jpeg)

图9-3　K-means算法与MiniBatchKmeans算法差异比较图

本案例代码如下。

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/21473765/1644303598650-1d1482da-e45f-4431-8f9c-38707e8791d0.jpeg)

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/21473765/1644303599333-a303fcde-015b-462d-984b-ac439e4051a7.jpeg)

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/21473765/1644303599650-a514497e-48f5-47ec-beef-737ed39d983d.jpeg)

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/21473765/1644303600224-2800680e-77fb-4f8e-aa60-d0e184797452.jpeg)

![img](https://cdn.nlark.com/yuque/0/2022/jpeg/21473765/1644303600576-fc458461-4c99-4cc3-8554-a2a2a2d09d1d.jpeg)
